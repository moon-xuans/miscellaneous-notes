# 操作系统

## 1.硬件结构

### 1.1.CPU是如何执行程序的？

> 64 位相⽐ 32 位 CPU 的优势在哪吗？64 位 CPU 的计算性能⼀定⽐ 32 位 CPU ⾼很多吗？

64 位相⽐ 32 位 CPU 的优势主要体现在两个⽅⾯：

- 64 位 CPU 可以⼀次计算超过 32 位的数字，⽽ 32 位 CPU 如果要计算超过 32 位的数字，要分多步骤进⾏计算，效率就没那么⾼，但是⼤部分应⽤程序很少会计算那么⼤的数字，所以只有运算⼤数字的时候，64 位 CPU 的优势才能体现出来，否则和 32 位 CPU 的计算性能相差不⼤。
- 64 位 CPU 可以寻址更⼤的内存空间，32 位 CPU 最⼤的寻址地址是 4G，即使你加了 8G ⼤⼩的内存，也还是只能寻址到 4G，⽽ 64 位 CPU 最⼤寻址地址是 2^64 ，远超于 32 位 CPU 最⼤寻址地址的 2^32 。



> 你知道软件的 32 位和 64 位之间的区别吗？再来 32 位的操作系统可以运⾏在 64 位的电脑上吗？64 位的操
> 作系统可以运⾏在 32 位的电脑上吗？如果不⾏，原因是什么？

- 如果 32 位指令在 64 位机器上执⾏，需要⼀套兼容机制，就可以做到兼容运⾏了。但是如果 64 位指令在 32 位机器上执⾏，就⽐较困难了，因为 32 位的寄存器存不下 64 位的指令；
- 操作系统其实也是⼀种程序，我们也会看到操作系统会分成 32 位操作系统、64 位操作系统，其代表意义就是操作系统中程序的指令是多少位，⽐如 64 位操作系统，指令也就是 64 位，因此不能装在32 位机器上。

### 1.2.存储器金字塔

各种存储器之间的关系，可以⽤我们在图书馆学习这个场景来理解。

CPU 可以⽐喻成我们的⼤脑，我们当前正在思考和处理的知识的过程，就好⽐ CPU 中的寄存器处理数据的过程，速度极快，但是容量很⼩。⽽ CPU 中的 L1-L3 Cache 好⽐我们⼤脑中的短期记忆和⻓期记忆，需要⼩⼩花费点时间来调取数据并处理。

我们⾯前的桌⼦就相当于内存，能放下更多的书（数据），但是找起来和看起来就要花费⼀些时间，相⽐CPU Cache 慢不少。⽽图书馆的书架相当于硬盘，能放下⽐内存更多的数据，但找起来就更费时间了，可以说是最慢的存储器设备了。

从 寄存器、CPU Cache，到内存、硬盘，这样⼀层层下来的存储器，访问速度越来越慢，存储容量越来越⼤，价格也越来越便宜，⽽且每个存储器只和相邻的⼀层存储器设备打交道，于是这样就形成了存储器的层次结构。

再来回答，开头的问题：那机械硬盘、固态硬盘、内存这三个存储器，到底和 CPU L1 Cache 相⽐速度差多少倍呢？

CPU L1 Cache 随机访问延时是 1 纳秒，内存则是 100 纳秒，所以 CPU L1 Cache ⽐内存快 100 倍左右。

SSD 随机访问延时是 150 微妙，所以 CPU L1 Cache ⽐ SSD 快 150000 倍左右。

最慢的机械硬盘随机访问延时已经⾼达 10 毫秒，我们来看看机械硬盘到底有多「⻳速」：

- SSD ⽐机械硬盘快 70 倍左右；
- 内存⽐机械硬盘快 100000 倍左右；
- CPU L1 Cache ⽐机械硬盘快 10000000 倍左右；

我们把上述的时间⽐例差异放⼤后，就能⾮常直观感受到它们的性能差异了。如果 CPU 访问 L1 Cache 的缓存时间是 1 秒，那访问内存则需要⼤约 2 分钟，随机访问 SSD ⾥的数据则需要 1.7 天，访问机械硬盘那更久，⻓达近 4 个⽉。

可以发现，不同的存储器之间性能差距很⼤，构造存储器分级很有意义，分级的⽬的是要构造缓存体系。

### 1.3.如何写出让CPU跑的更快的代码？

由于随着计算机技术的发展，CPU 与 内存的访问速度相差越来越多，如今差距已经⾼达好⼏百倍了，所以CPU 内部嵌⼊了 CPU Cache 组件，作为内存与 CPU 之间的缓存层，CPU Cache 由于离 CPU 核⼼很近，所以访问速度也是⾮常快的，但由于所需材料成本⽐较⾼，它不像内存动辄⼏个 GB ⼤⼩，⽽是仅有⼏⼗ KB 到 MB ⼤⼩。

当 CPU 访问数据的时候，先是访问 CPU Cache，如果缓存命中的话，则直接返回数据，就不⽤每次都从内存读取速度了。因此，缓存命中率越⾼，代码的性能越好。

但需要注意的是，当 CPU 访问数据时，如果 CPU Cache 没有缓存该数据，则会从内存读取数据，但是并不是只读⼀个数据，⽽是⼀次性读取⼀块⼀块的数据存放到 CPU Cache 中，之后才会被 CPU 读取。

内存地址映射到 CPU Cache 地址⾥的策略有很多种，其中⽐较简单是直接映射 Cache，它巧妙的把内存地址拆分成「索引 + 组标记 + 偏移量」的⽅式，使得我们可以将很⼤的内存地址，映射到很⼩的 CPUCache 地址⾥。

要想写出让 CPU 跑得更快的代码，就需要写出缓存命中率⾼的代码，CPU L1 Cache 分为数据缓存和指令缓存，因⽽需要分别提⾼它们的缓存命中率：

- 对于数据缓存，我们在遍历数据的时候，应该按照内存布局的顺序操作，这是因为 CPU Cache 是根据 CPU Cache Line 批量操作数据的，所以顺序地操作连续内存数据时，性能能得到有效的提升；
- 对于指令缓存，有规律的条件分⽀语句能够让 CPU 的分⽀预测器发挥作⽤，进⼀步提⾼执⾏的效率；

另外，对于多核 CPU 系统，线程可能在不同 CPU 核⼼来回切换，这样各个核⼼的缓存命中率就会受到影响，于是要想提⾼进程的缓存命中率，可以考虑把线程绑定 CPU 到某⼀个 CPU 核⼼。

### 1.4.CPU缓存一致性

CPU 在读写数据的时候，都是在 CPU Cache 读写数据的，原因是 Cache 离 CPU 很近，读写性能相⽐内存⾼出很多。对于 Cache ⾥没有缓存 CPU 所需要读取的数据的这种情况，CPU 则会从内存读取数据，并将数据缓存到 Cache ⾥⾯，最后 CPU 再从 Cache 读取数据。

⽽对于数据的写⼊，CPU 都会先写⼊到 Cache ⾥⾯，然后再在找个合适的时机写⼊到内存，那就有「写直达」和「写回」这两种策略来保证 Cache 与内存的数据⼀致性：

- 写直达，只要有数据写⼊，都会直接把数据写⼊到内存⾥⾯，这种⽅式简单直观，但是性能就会受限于内存的访问速度；
- 写回，对于已经缓存在 Cache 的数据的写⼊，只需要更新其数据就可以，不⽤写⼊到内存，只有在需要把缓存⾥⾯的脏数据交换出去的时候，才把数据同步到内存⾥，这种⽅式在缓存命中率⾼的情况，性能会更好；

当今 CPU 都是多核的，每个核⼼都有各⾃独⽴的 L1/L2 Cache，只有 L3 Cache 是多个核⼼之间共享的。所以，我们要确保多核缓存是⼀致性的，否则会出现错误的结果。

要想实现缓存⼀致性，关键是要满⾜ 2 点：

- 第⼀点是写传播，也就是当某个 CPU 核⼼发⽣写⼊操作时，需要把该事件⼴播通知给其他核⼼；
- 第⼆点是事物的串⾏化，这个很重要，只有保证了这个，才能保障我们的数据是真正⼀致的，我们的程序在各个不同的核⼼上运⾏的结果也是⼀致的；

基于总线嗅探机制的 MESI 协议，就满⾜上⾯了这两点，因此它是保障缓存⼀致性的协议。

MESI 协议，是已修改、独占、共享、已失效这四个状态的英⽂缩写的组合。整个 MESI 状态的变更，则是根据来⾃本地 CPU 核⼼的请求，或者来⾃其他 CPU 核⼼通过总线传输过来的请求，从⽽构成⼀个流动的状态机。另外，对于在「已修改」或者「独占」状态的 Cache Line，修改更新其数据不需要发送⼴播给其他 CPU 核⼼。

### 1.5.CPU是如何执行任务的？

理解 CPU 是如何读写数据的前提，是要理解 CPU 的架构，CPU 内部的多个 Cache + 外部的内存和磁盘都就构成了⾦字塔的存储器结构，在这个⾦字塔中，越往下，存储器的容量就越⼤，但访问速度就会⼩。

CPU 读写数据的时候，并不是按⼀个⼀个字节为单位来进⾏读写，⽽是以 CPU Line ⼤⼩为单位，CPULine ⼤⼩⼀般是 64 个字节，也就意味着 CPU 读写数据的时候，每⼀次都是以 64 字节⼤⼩为⼀块进⾏操作。

因此，如果我们操作的数据是数组，那么访问数组元素的时候，按内存分布的地址顺序进⾏访问，这样能充分利⽤到 Cache，程序的性能得到提升。但如果操作的数据不是数组，⽽是普通的变量，并在多核 CPU的情况下，我们还需要避免 Cache Line 伪共享的问题。

所谓的 Cache Line 伪共享问题就是，多个线程同时读写同⼀个 Cache Line 的不同变量时，⽽导致 CPUCache 失效的现象。那么对于多个线程共享的热点数据，即经常会修改的数据，应该避免这些数据刚好在同⼀个 Cache Line 中，避免的⽅式⼀般有 Cache Line ⼤⼩字节对⻬，以及字节填充等⽅法。

系统中需要运⾏的多线程数⼀般都会⼤于 CPU 核⼼，这样就会导致线程排队等待 CPU，这可能会产⽣⼀定的延时，如果我们的任务对延时容忍度很低，则可以通过⼀些⼈为⼿段⼲预 Linux 的默认调度策略和优先级。

### 1.6.软中断

为了避免由于中断处理程序执⾏时间过⻓，⽽影响正常进程的调度，Linux 将中断处理程序分为上半部和下半部：

- 上半部，对应硬中断，由硬件触发中断，⽤来快速处理中断；
- 下半部，对应软中断，由内核触发中断，⽤来异步处理上半部未完成的⼯作；

Linux 中的软中断包括⽹络收发、定时、调度、RCU 锁等各种类型，可以通过查看 /proc/softirqs 来观察软中断的累计中断次数情况，如果要实时查看中断次数的变化率，可以使⽤ watch -d cat /proc/softirqs 命令。

每⼀个 CPU 都有各⾃的软中断内核线程，我们还可以⽤ ps 命令来查看内核线程，⼀般名字在中括号⾥⾯，都认为是内核线程。

如果在 top 命令发现，CPU 在软中断上的使⽤率⽐较⾼，⽽且 CPU 使⽤率最⾼的进程也是软中断ksoftirqd 的时候，这种⼀般可以认为系统的开销被软中断占据了。

这时我们就可以分析是哪种软中断类型导致的，⼀般来说都是因为⽹络接收软中断导致的，如果是的话，可以⽤ sar 命令查看是哪个⽹卡的有⼤量的⽹络包接收，再⽤ tcpdump 抓⽹络包，做进⼀步分析该⽹络包的源头是不是⾮法地址，如果是就需要考虑防⽕墙增加规则，如果不是，则考虑硬件升级等。

### 1.7.为什么0.1 + 0.2 不等于0.3？

> 为什么负数要⽤补码表示？

负数之所以⽤补码的⽅式来表示，主要是为了统⼀和正数的加减法操作⼀样，毕竟数字的加减法是很常⽤的⼀个操作，就不要搞特殊化，尽量以统⼀的⽅式来运算。

> ⼗进制⼩数怎么转成⼆进制？

⼗进制整数转⼆进制使⽤的是「除 2 取余法」，⼗进制⼩数使⽤的是「乘 2 取整法」。

> 计算机是怎么存⼩数的？

计算机是以浮点数的形式存储⼩数的，⼤多数计算机都是 IEEE 754 标准定义的浮点数格式，包含三个部分：

- 符号位：表示数字是正数还是负数，为 0 表示正数，为 1 表示负数；
- 指数位：指定了⼩数点在数据中的位置，指数可以是负数，也可以是正数，指数位的⻓度越⻓则数值的表达范围就越⼤；
- 尾数位：⼩数点右侧的数字，也就是⼩数部分，⽐如⼆进制 1.0011 x 2^(-2)，尾数部分就是 0011，⽽且尾数的⻓度决定了这个数的精度，因此如果要表示精度更⾼的⼩数，则就要提⾼尾数位的⻓度；

⽤ 32 位来表示的浮点数，则称为单精度浮点数，也就是我们编程语⾔中的 float 变量，⽽⽤ 64 位来表示的浮点数，称为双精度浮点数，也就是 double 变量。

> 0.1 + 0.2 == 0.3 吗？

不是的，0.1 和 0.2 这两个数字⽤⼆进制表达会是⼀个⼀直循环的⼆进制数，⽐如 0.1 的⼆进制表示为 0.00011 0011 0011… （0011 ⽆限循环)，对于计算机⽽⾔，0.1 ⽆法精确表达，这是浮点数计算造成精度损失的根源。

因此，IEEE 754 标准定义的浮点数只能根据精度舍⼊，然后⽤「近似值」来表示该⼆进制，那么意味着计算机存放的⼩数可能不是⼀个真实值。

0.1 + 0.2 并不等于完整的 0.3，这主要是因为这两个⼩数⽆法⽤「完整」的⼆进制来表示，只能根据精度舍⼊，所以计算机⾥只能采⽤近似数的⽅式来保存，那两个近似数相加，得到的必然也是⼀个近似数。

## 2.操作系统结构

对于内核的架构⼀般有这三种类型：

- 宏内核，包含多个模块，整个内核像⼀个完整的程序；
- 微内核，有⼀个最⼩版本的内核，⼀些模块和服务则由⽤户态管理；
- 混合内核，是宏内核和微内核的结合体，内核中抽象出了微内核的概念，也就是内核中会有⼀个⼩型的内核，其他模块就在这个基础上搭建，整个内核是个完整的程序；

Linux 的内核设计是采⽤了宏内核，Window 的内核设计则是采⽤了混合内核。

这两个操作系统的可执⾏⽂件格式也不⼀样， Linux 可执⾏⽂件格式叫作 ELF，Windows 可执⾏⽂件格式叫作 PE。

## 3.内存管理-虚拟内存

为了在多进程环境下，使得进程之间的内存地址不受影响，相互隔离，于是操作系统就为每个进程独⽴分配⼀套虚拟地址空间，每个程序只关⼼⾃⼰的虚拟地址就可以，实际上⼤家的虚拟地址都是⼀样的，但分布到物理地址内存是不⼀样的。作为程序，也不⽤关⼼物理地址的事情。

每个进程都有⾃⼰的虚拟空间，⽽物理内存只有⼀个，所以当启⽤了⼤量的进程，物理内存必然会很紧张，于是操作系统会通过内存交换技术，把不常使⽤的内存暂时存放到硬盘（换出），在需要的时候再装载回物理内存（换⼊）。

那既然有了虚拟地址空间，那必然要把虚拟地址「映射」到物理地址，这个事情通常由操作系统来维护。

那么对于虚拟地址与物理地址的映射关系，可以有分段和分⻚的⽅式，同时两者结合都是可以的。

内存分段是根据程序的逻辑⻆度，分成了栈段、堆段、数据段、代码段等，这样可以分离出不同属性的段，同时是⼀块连续的空间。但是每个段的⼤⼩都不是统⼀的，这就会导致内存碎⽚和内存交换效率低的问题。

于是，就出现了内存分⻚，把虚拟空间和物理空间分成⼤⼩固定的⻚，如在 Linux 系统中，每⼀⻚的⼤⼩为 4KB 。由于分了⻚后，就不会产⽣细⼩的内存碎⽚。同时在内存交换的时候，写⼊硬盘也就⼀个⻚或⼏个⻚，这就⼤⼤提⾼了内存交换的效率。

再来，为了解决简单分⻚产⽣的⻚表过⼤的问题，就有了多级⻚表，它解决了空间上的问题，但这就会导致 CPU 在寻址的过程中，需要有很多层表参与，加⼤了时间上的开销。于是根据程序的局部性原理，在CPU 芯⽚中加⼊了 TLB，负责缓存最近常被访问的⻚表项，⼤⼤提⾼了地址的转换速度。

Linux 系统主要采⽤了分⻚管理，但是由于 Intel 处理器的发展史，Linux 系统⽆法避免分段管理。于是Linux 就把所有段的基地址设为 0 ，也就意味着所有程序的地址空间都是线性地址空间（虚拟地址），相当于屏蔽了 CPU 逻辑地址的概念，所以段只被⽤于访问控制和内存保护。

另外，Linxu 系统中虚拟空间分布可分为⽤户态和内核态两部分，其中⽤户态的分布：代码段、全局变量、BSS、函数栈、堆内存、映射区。

## 4.进程与线程

### 4.1.进程、线程基础知识

#### 4.1.1.进程

我们编写的代码只是⼀个存储在硬盘的静态⽂件，通过编译后就会⽣成⼆进制可执⾏⽂件，当我们运⾏这个可执⾏⽂件后，它会被装载到内存中，接着 CPU 会执⾏程序中的每⼀条指令，那么这个运⾏中的程序，就被称为「进程」（Process）。

##### 4.1.1.1.进程的状态

在一个进程的活动期间至少具备三种基本状态，即运行状态、就绪状态、阻塞状态。

- 运⾏状态（Runing）：该时刻进程占⽤ CPU；
- 就绪状态（Ready）：可运⾏，由于其他进程处于运⾏状态⽽暂时停⽌运⾏；
- 阻塞状态（Blocked）：该进程正在等待某⼀事件发⽣（如等待输⼊/输出操作的完成）⽽暂时停⽌运⾏，这时，即使给它CPU控制权，它也⽆法运⾏；

进程还有另外两个基本状态:

- 创建状态（new）：进程正在被创建时的状态；
- 结束状态（Exit）：进程正在从系统中消失时的状态；

如果有大量处于阻塞状态的进程，进程可能会占用物理内存空间，显然不是我们所希望的，毕竟物理内存空间是有限的，被阻塞状态的进程占用着物理内存就一种浪费物理内存的行为。

所以，在虚拟内存管理的操作系统中，通常会把阻塞状态的进程的物理内存换出硬盘，等需要再次运行的时候，再从硬盘换入到物理内存。

那么，就需要一个新的状态，来描述进程没有占用实际的物理内存空间的情况，这个状态就是挂起状态。这跟阻塞状态是不一样，阻塞状态是等待某个事件的返回。

另外，挂起状态可以分为两种：

- 阻塞挂起状态：进程在外存(硬盘)并等待某个事件的出现；
- 就绪挂起状态：进程在外存(硬盘)，但只要进入内存，即刻立即运行；

所以完整的进程状态变迁，如图:

![](https://moon-axuan.oss-cn-beijing.aliyuncs.com/飞书20220709-084548.png)

- NULL -> 创建状态：⼀个新进程被创建时的第⼀个状态；
- 创建状态 -> 就绪状态：当进程被创建完成并初始化后，⼀切就绪准备运⾏时，变为就绪状态，这个过程是很快的；
- 就绪态 -> 运⾏状态：处于就绪状态的进程被操作系统的进程调度器选中后，就分配给 CPU 正式运⾏该进程；
- 运⾏状态 -> 结束状态：当进程已经运⾏完成或出错时，会被操作系统作结束状态处理；
- 运⾏状态 -> 就绪状态：处于运⾏状态的进程在运⾏过程中，由于分配给它的运⾏时间⽚⽤完，操作系统会把该进程变为就绪态，接着从就绪态选中另外⼀个进程运⾏；
- 运⾏状态 -> 阻塞状态：当进程请求某个事件且必须等待时，例如请求 I/O 事件；
- 阻塞状态 -> 就绪状态：当进程要等待的事件完成时，它从阻塞状态变到就绪状态；



##### 4.1.1.2.进程的上下文切换

进程是由内核管理和调度的，所以进程的切换只能发生在内核态。

所以，进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。

通常，会把交换的信息保存在进程的PCB，当要运行另外一个进程的时候，我们需要从这个进程的PCB取出上下文，然后恢复到CPU中，使得这个进程可以继续执行。

#### 4.1.2.线程

##### 4.1.2.1.为什么使用线程？

进程之间的通信，比较麻烦，而线程之间可以并发运行且共享相同的地址空间。

##### 4.1.2.2.什么是线程？

线程是进程当中的一条执行流程。

同一个进程内多个线程之间可以共享代码段、数据段、打开的文件等资源，但每个线程各自都有一套独立的寄存器和栈，这样可以确保线程的控制流是相对独立的。

> 线程的优缺点？

线程的优点:

- 一个进程中可以同时存在多个线程；
- 各个线程之间可以并发执行；
- 各个线程之间可以共享地址空间和文件等资源；

线程的缺点：

- 当进程中的一个线程崩溃时，会导致其所属进程的所有线程崩溃。

##### 4.1.2.3.线程与进程的比较

线程与进程的⽐较如下：

- 进程是资源（包括内存、打开的⽂件等）分配的单位，线程是 CPU 调度的单位；
- 进程拥有⼀个完整的资源平台，⽽线程只独享必不可少的资源，如寄存器和栈；
- 线程同样具有就绪、阻塞、执⾏三种基本状态，同样具有状态之间的转换关系；
- 线程能减少并发执⾏的时间和空间开销；

对于，线程相⽐进程能减少开销，体现在：

- 线程的创建时间⽐进程快，因为进程在创建的过程中，还需要资源管理信息，⽐如内存管理信息、⽂件管理信息，⽽线程在创建的过程中，不会涉及这些资源管理信息，⽽是共享它们；
- 线程的终⽌时间⽐进程快，因为线程释放的资源相⽐进程少很多；
- 同⼀个进程内的线程切换⽐进程切换快，因为线程具有相同的地址空间（虚拟内存共享），这意味着同⼀个进程的线程都具有同⼀个⻚表，那么在切换的时候不需要切换⻚表。⽽对于进程之间的切换，切换的时候要把⻚表给切换掉，⽽⻚表的切换过程开销是⽐较⼤的；
- 由于同⼀进程的各线程间共享内存和⽂件资源，那么在线程之间数据传递的时候，就不需要经过内核了，这就使得线程之间的数据交互效率更⾼了；

所以，不管是时间效率，还是空间效率线程⽐进程都要⾼。

### 4.2.进程间通信

由于每个进程的⽤户空间都是独⽴的，不能相互访问，这时就需要借助内核空间来实现进程间通信，原因很简单，每个进程都是共享⼀个内核空间。

Linux 内核提供了不少进程间通信的⽅式，其中最简单的⽅式就是管道，管道分为「匿名管道」和「命名管道」。

匿名管道顾名思义，它没有名字标识，匿名管道是特殊⽂件只存在于内存，没有存在于⽂件系统中，shell命令中的「 | 」竖线就是匿名管道，通信的数据是⽆格式的流并且⼤⼩受限，通信的⽅式是单向的，数据只能在⼀个⽅向上流动，如果要双向通信，需要创建两个管道，再来匿名管道是只能⽤于存在⽗⼦关系的进程间通信，匿名管道的⽣命周期随着进程创建⽽建⽴，随着进程终⽌⽽消失。

命名管道突破了匿名管道只能在亲缘关系进程间的通信限制，因为使⽤命名管道的前提，需要在⽂件系统创建⼀个类型为 p 的设备⽂件，那么毫⽆关系的进程就可以通过这个设备⽂件进⾏通信。另外，不管是匿名管道还是命名管道，进程写⼊的数据都是缓存在内核中，另⼀个进程读取数据时候⾃然也是从内核中获取，同时通信数据都遵循先进先出原则，不⽀持 lseek 之类的⽂件定位操作。

消息队列克服了管道通信的数据是⽆格式的字节流的问题，消息队列实际上是保存在内核的「消息链表」，消息队列的消息体是可以⽤户⾃定义的数据类型，发送数据时，会被分成⼀个⼀个独⽴的消息体，当然接收数据时，也要与发送⽅发送的消息体的数据类型保持⼀致，这样才能保证读取的数据是正确的。消息队列通信的速度不是最及时的，毕竟每次数据的写⼊和读取都需要经过⽤户态与内核态之间的拷⻉过程。

共享内存可以解决消息队列通信中⽤户态与内核态之间数据拷⻉过程带来的开销，它直接分配⼀个共享空间，每个进程都可以直接访问，就像访问进程⾃⼰的空间⼀样快捷⽅便，不需要陷⼊内核态或者系统调⽤，⼤⼤提⾼了通信的速度，享有最快的进程间通信⽅式之名。但是便捷⾼效的共享内存通信，带来新的
问题，多进程竞争同个共享资源会造成数据的错乱。

那么，就需要信号量来保护共享资源，以确保任何时刻只能有⼀个进程访问共享资源，这种⽅式就是互斥访问。信号量不仅可以实现访问的互斥性，还可以实现进程间的同步，信号量其实是⼀个计数器，表示的是资源个数，其值可以通过两个原⼦操作来控制，分别是 P 操作和 V 操作。

与信号量名字很相似的叫信号，它俩名字虽然相似，但功能⼀点⼉都不⼀样。信号是进程间通信机制中唯⼀的异步通信机制，信号可以在应⽤进程和内核之间直接交互，内核也可以利⽤信号来通知⽤户空间的进程发⽣了哪些系统事件，信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令），⼀旦有信号发⽣，进程有三种⽅式响应信号 1. 执⾏默认操作、2. 捕捉信号、3. 忽略信号。有两个信号是应⽤进程⽆法捕捉和忽略的，即 SIGKILL 和 SEGSTOP ，这是为了⽅便我们能在任何时候结束或停⽌某个进程。

前⾯说到的通信机制，都是⼯作于同⼀台主机，如果要与不同主机的进程间通信，那么就需要 Socket 通信了。Socket 实际上不仅⽤于不同的主机进程间通信，还可以⽤于本地主机进程间通信，可根据创建Socket 的类型不同，分为三种常⻅的通信⽅式，⼀个是基于 TCP 协议的通信⽅式，⼀个是基于 UDP 协议
的通信⽅式，⼀个是本地进程间通信⽅式。

以上，就是进程间通信的主要机制了。你可能会问了，那线程通信间的⽅式呢？

同个进程下的线程之间都是共享进程的资源，只要是共享变量都可以做到线程间通信，⽐如全局变量，所以对于线程间关注的不是通信⽅式，⽽是关注多线程竞争共享资源的问题，信号量也同样可以在线程间实现互斥与同步：

- 互斥的⽅式，可保证任意时刻只有⼀个线程访问共享资源；
- 同步的⽅式，可保证线程 A 应在线程 B 之前执⾏；

### 4.3.死锁

简单来说，死锁问题的产⽣是由两个或者以上线程并⾏执⾏的时候，争夺资源⽽互相等待造成的。

死锁只有同时满⾜互斥、持有并等待、不可剥夺、环路等待这四个条件的时候才会发⽣。

所以要避免死锁问题，就是要破坏其中⼀个条件即可，最常⽤的⽅法就是使⽤资源有序分配法来破坏环路等待条件。

### 4.4.乐观锁与悲观锁

开发过程中，最常⻅的就是互斥锁的了，互斥锁加锁失败时，会⽤「线程切换」来应对，当加锁失败的线程再次加锁成功后的这⼀过程，会有两次线程上下⽂切换的成本，性能损耗⽐较⼤。

如果我们明确知道被锁住的代码的执⾏时间很短，那我们应该选择开销⽐较⼩的⾃旋锁，因为⾃旋锁加锁失败时，并不会主动产⽣线程切换，⽽是⼀直忙等待，直到获取到锁，那么如果被锁住的代码执⾏时间很短，那这个忙等待的时间相对应也很短。

如果能区分读操作和写操作的场景，那读写锁就更合适了，它允许多个读线程可以同时持有读锁，提⾼了读的并发性。根据偏袒读⽅还是写⽅，可以分为读优先锁和写优先锁，读优先锁并发性很强，但是写线程会被饿死，⽽写优先锁会优先服务写线程，读线程也可能会被饿死，那为了避免饥饿的问题，于是就有了公平读写锁，它是⽤队列把请求锁的线程排队，并保证先⼊先出的原则来对线程加锁，这样便保证了某种线程不会被饿死，通⽤性也更好点。

互斥锁和⾃旋锁都是最基本的锁，读写锁可以根据场景来选择这两种锁其中的⼀个进⾏实现。

另外，互斥锁、⾃旋锁、读写锁都属于悲观锁，悲观锁认为并发访问共享资源时，冲突概率可能⾮常⾼，所以在访问共享资源前，都需要先加锁。

相反的，如果并发访问共享资源时，冲突概率⾮常低的话，就可以使⽤乐观锁，它的⼯作⽅式是，在访问共享资源时，不⽤先加锁，修改完共享资源后，再验证这段时间内有没有发⽣冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作。

但是，⼀旦冲突概率上升，就不适合使⽤乐观锁了，因为它解决冲突的重试成本⾮常⾼。

不管使⽤的哪种锁，我们的加锁的代码范围应该尽可能的⼩，也就是加锁的粒度要⼩，这样执⾏速度会⽐较快。再来，使⽤上了合适的锁，就会快上加快了。

## 5.调度算法

### 5.1.进程调度算法

进程调度算法也称 CPU 调度算法，毕竟进程是由 CPU 调度的。

当 CPU 空闲时，操作系统就选择内存中的某个「就绪状态」的进程，并给其分配 CPU。

#### 5.1.1.先来先服务调度算法

最简单的⼀个调度算法，就是⾮抢占式的先来先服务（First Come First Severd, FCFS）算法了。

顾名思义，先来后到，每次从就绪队列选择最先进⼊队列的进程，然后⼀直运⾏，直到进程退出或被阻塞，才会继续从队列中选择第⼀个进程接着运⾏。

#### 5.1.2.最短作业优先调度算法

最短作业优先（Shortest Job First, SJF）调度算法同样也是顾名思义，它会优先选择运⾏时间最短的进程来运⾏，这有助于提⾼系统的吞吐量。

#### 5.1.3.高响应优先调度算法

⾼响应⽐优先（Highest Response Ratio Next, HRRN）调度算法主要是权衡了短作业和⻓作业。

每次进⾏进程调度时，先计算「响应⽐优先级」，然后把「响应⽐优先级」最⾼的进程投⼊运⾏。

优先权 = (等待时间 + 要求服务时间) / 要求服务时间

#### 5.1.4.时间片轮转调度算法

最古⽼、最简单、最公平且使⽤最⼴的算法就是时间⽚轮转（Round Robin, RR）调度算法。

每个进程被分配⼀个时间段，称为时间⽚（Quantum），即允许该进程在该时间段中运⾏。

- 如果时间⽚⽤完，进程还在运⾏，那么将会把此进程从 CPU 释放出来，并把 CPU 分配另外⼀个进程；
- 如果该进程在时间⽚结束前阻塞或结束，则 CPU ⽴即进⾏切换；

#### 5.1.5.最高优先级调度算法

前⾯的「时间⽚轮转算法」做了个假设，即让所有的进程同等重要，也不偏袒谁，⼤家的运⾏时间都⼀样。

但是，对于多⽤户计算机系统就有不同的看法了，它们希望调度是有优先级的，即希望调度程序能从就绪队列中选择最⾼优先级的进程进⾏运⾏，这称为最⾼优先级（Highest Priority First ， HPF）调度算法。

进程的优先级可以分为，静态优先级或动态优先级：

- 静态优先级：创建进程时候，就已经确定了优先级了，然后整个运⾏时间优先级都不会变化；
- 动态优先级：根据进程的动态变化调整优先级，⽐如如果进程运⾏时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升⾼其优先级，也就是随着时间的推移增加等待进程的优先级。

该算法也有两种处理优先级⾼的⽅法，⾮抢占式和抢占式：

- ⾮抢占式：当就绪队列中出现优先级⾼的进程，运⾏完当前进程，再选择优先级⾼的进程。
- 抢占式：当就绪队列中出现优先级⾼的进程，当前进程挂起，调度优先级⾼的进程运⾏。

但是依然有缺点，可能会导致低优先级的进程永远不会运⾏。

#### 5.1.6.多级反馈队列调度算法

多级反馈队列（Multilevel Feedback Queue）调度算法是「时间⽚轮转算法」和「最⾼优先级算法」的综合和发展。

顾名思义：

- 「多级」表示有多个队列，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越短。
- 「反馈」表示如果有新的进程加⼊优先级⾼的队列时，⽴刻停⽌当前正在运⾏的进程，转⽽去运⾏优先级⾼的队列；

来看看，它是如何⼯作的：

- 设置了多个队列，赋予每个队列不同的优先级，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越短；
- 新的进程会被放⼊到第⼀级队列的末尾，按先来先服务的原则排队等待被调度，如果在第⼀级队列规定的时间⽚没运⾏完成，则将其转⼊到第⼆级队列的末尾，以此类推，直⾄完成；
- 当较⾼优先级的队列为空，才调度较低优先级的队列中的进程运⾏。如果进程运⾏时，有新进程进⼊较⾼优先级的队列，则停⽌当前运⾏的进程并将其移⼊到原队列末尾，接着让较⾼优先级的进程运⾏；

可以发现，对于短作业可能可以在第⼀级队列很快被处理完。对于⻓作业，如果在第⼀级队列处理不完，可以移⼊下次队列等待被执⾏，虽然等待的时间变⻓了，但是运⾏时间也会更⻓了，所以该算法很好的兼顾了⻓短作业，同时有较好的响应时间。

### 5.2.内存页面置换算法

⻚⾯置换算法的功能是，当出现缺⻚异常，需调⼊新⻚⾯⽽内存已满时，选择被置换的物理⻚⾯，也就是说选择⼀个物理⻚⾯换出到磁盘，然后把需要访问的⻚⾯换⼊到物理⻚。

#### 5.2.1.最佳页面置换算法

最佳⻚⾯置换算法基本思路是，置换在「未来」最⻓时间不访问的⻚⾯。

这很理想，但是实际系统中⽆法实现，因为程序访问⻚⾯时是动态的，我们是⽆法预知每个⻚⾯在「下⼀次」访问前的等待时间。

所以，最佳⻚⾯置换算法作⽤是为了衡量你的算法的效率，你的算法效率越接近该算法的效率，那么说明你的算法是⾼效的。

#### 5.2.2.先进先出置换算法

既然我们⽆法预知⻚⾯在下⼀次访问前所需的等待时间，那我们可以选择在内存驻留时间很⻓的⻚⾯进⾏中置换，这个就是「先进先出置换」算法的思想。

#### 5.2.3.最近最久未使用的置换算法

最近最久未使⽤（LRU）的置换算法的基本思路是，发⽣缺⻚时，选择最⻓时间没有被访问的⻚⾯进⾏置换，也就是说，该算法假设已经很久没有使⽤的⻚⾯很有可能在未来较⻓的⼀段时间内仍然不会被使⽤。

这种算法近似最优置换算法，最优置换算法是通过「未来」的使⽤情况来推测要淘汰的⻚⾯，⽽ LRU 则是通过「历史」的使⽤情况来推测要淘汰的⻚⾯。

虽然 LRU 在理论上是可以实现的，但代价很⾼。为了完全实现 LRU，需要在内存中维护⼀个所有⻚⾯的链表，最近最多使⽤的⻚⾯在表头，最近最少使⽤的⻚⾯在表尾。

困难的是，在每次访问内存时都必须要更新「整个链表」。在链表中找到⼀个⻚⾯，删除它，然后把它移动到表头是⼀个⾮常费时的操作。

所以，LRU 虽然看上去不错，但是由于开销⽐较⼤，实际应⽤中⽐较少使⽤。

#### 5.2.4.时钟页面置换算法

那有没有⼀种即能优化置换的次数，也能⽅便实现的算法呢？

时钟⻚⾯置换算法就可以两者兼得，它跟 LRU 近似，⼜是对 FIFO 的⼀种改进。

该算法的思路是，把所有的⻚⾯都保存在⼀个类似钟⾯的「环形链表」中，⼀个表针指向最⽼的⻚⾯。

当发⽣缺⻚中断时，算法⾸先检查表针指向的⻚⾯：

- 如果它的访问位位是 0 就淘汰该⻚⾯，并把新的⻚⾯插⼊这个位置，然后把表针前移⼀个位置；
- 如果访问位是 1 就清除访问位，并把表针前移⼀个位置，重复这个过程直到找到了⼀个访问位为 0 的⻚⾯为⽌；

#### 5.2.5.最不常用算法

最不常⽤（LFU）算法，这名字听起来很调⽪，但是它的意思不是指这个算法不常⽤，⽽是当发⽣缺⻚中断时，选择「访问次数」最少的那个⻚⾯，并将其淘汰。

它的实现⽅式是，对每个⻚⾯设置⼀个「访问计数器」，每当⼀个⻚⾯被访问时，该⻚⾯的访问计数器就累加 1。在发⽣缺⻚中断时，淘汰计数器值最⼩的那个⻚⾯。

看起来很简单，每个⻚⾯加⼀个计数器就可以实现了，但是在操作系统中实现的时候，我们需要考虑效率和硬件成本的。

要增加⼀个计数器来实现，这个硬件成本是⽐较⾼的，另外如果要对这个计数器查找哪个⻚⾯访问次数最⼩，查找链表本身，如果链表⻓度很⼤，是⾮常耗时的，效率不⾼。

但还有个问题，LFU 算法只考虑了频率问题，没考虑时间的问题，⽐如有些⻚⾯在过去时间⾥访问的频率很⾼，但是现在已经没有访问了，⽽当前频繁访问的⻚⾯由于没有这些⻚⾯访问的次数⾼，在发⽣缺⻚中断时，就会可能会误伤当前刚开始频繁访问，但访问次数还不⾼的⻚⾯。

那这个问题的解决的办法还是有的，可以定期减少访问的次数，⽐如当发⽣时间中断时，把过去时间访问的⻚⾯的访问次数除以 2，也就说，随着时间的流失，以前的⾼访问次数的⻚⾯会慢慢减少，相当于加⼤了被置换的概率。

### 5.3.磁盘调度算法

磁盘调度算法的⽬的很简单，就是为了提⾼磁盘的访问性能，⼀般是通过优化磁盘的访问请求顺序来做到的。

寻道的时间是磁盘访问最耗时的部分，如果请求顺序优化的得当，必然可以节省⼀些不必要的寻道时间，从⽽提⾼磁盘的访问性能。

#### 5.3.1.先来先服务

先来先服务（First-Come ， First-Served ， FCFS），顾名思义，先到来的请求，先被服务。

这种算法，⽐较简单粗暴，但是如果⼤量进程竞争使⽤磁盘，请求访问的磁道可能会很分散，那先来先服务算法在性能上就会显得很差，因为寻道时间过⻓。

#### 5.3.2.扫描算法

最短寻道时间优先算法会产⽣饥饿的原因在于：磁头有可能在⼀个⼩区域内来回得移动。

为了防⽌这个问题，可以规定：磁头在⼀个⽅向上移动，访问所有未完成的请求，直到磁头到达该⽅向上的最后的磁道，才调换⽅向，这就是扫描（Scan）算法。

扫描调度算法性能较好，不会产⽣饥饿现象，但是存在这样的问题，中间部分的磁道会⽐较占便宜，中间部分相⽐其他部分响应的频率会⽐较多，也就是说每个磁道的响应频率存在差异。

#### 5.3.3.循环扫描算法

扫描算法使得每个磁道响应的频率存在差异，那么要优化这个问题的话，可以总是按相同的⽅向进⾏扫描，使得每个磁道的响应频率基本⼀致。

循环扫描（Circular Scan, CSCAN ）规定：只有磁头朝某个特定⽅向移动时，才处理磁道访问请求，⽽返回时直接快速移动⾄最靠边缘的磁道，也就是复位磁头，这个过程是很快的，并且返回中途不处理任何请求，该算法的特点，就是磁道只响应⼀个⽅向上的请求。

循环扫描算法相⽐于扫描算法，对于各个位置磁道响应频率相对⽐较平均。

#### 5.3.4.LOOK与C-LOOK算法

我们前⾯说到的扫描算法和循环扫描算法，都是磁头移动到磁盘「最始端或最末端」才开始调换⽅向。

那这其实是可以优化的，优化的思路就是磁头在移动到「最远的请求」位置，然后⽴即反向移动。

那针对 SCAN 算法的优化则叫 LOOK 算法，它的⼯作⽅式，磁头在每个⽅向上仅仅移动到最远的请求位置，然后⽴即反向移动，⽽不需要移动到磁盘的最始端或最末端，反向移动的途中会响应请求。

⽽针 C-SCAN 算法的优化则叫 C-LOOK，它的⼯作⽅式，磁头在每个⽅向上仅仅移动到最远的请求位置，然后⽴即反向移动，⽽不需要移动到磁盘的最始端或最末端，反向移动的途中不会响应请求。

## 6.文件系统

### 6.1.文件系统的基本组成

⽂件系统是操作系统中负责管理持久数据的⼦系统，说简单点，就是负责把⽤户的⽂件存到磁盘硬件中，因为即使计算机断电了，磁盘⾥的数据并不会丢失，所以可以持久化的保存⽂件。

⽂件系统的基本数据单位是⽂件，它的⽬的是对磁盘上的⽂件进⾏组织管理，那组织的⽅式不同，就会形成不同的⽂件系统。

Linux 最经典的⼀句话是：「⼀切皆⽂件」，不仅普通的⽂件和⽬录，就连块设备、管道、socket 等，也都是统⼀交给⽂件系统管理的。

### 6.2.虚拟文件系统

⽂件系统的种类众多，⽽操作系统希望对⽤户提供⼀个统⼀的接⼝，于是在⽤户层与⽂件系统层引⼊了中间层，这个中间层就称为虚拟⽂件系统（Virtual File System ， VFS）。

VFS 定义了⼀组所有⽂件系统都⽀持的数据结构和标准接⼝，这样程序员不需要了解⽂件系统的⼯作原理，只需要了解 VFS 提供的统⼀接⼝即可。

### 6.3.文件的使用

![](https://moon-axuan.oss-cn-beijing.aliyuncs.com/飞书20220709-105603.png)

### 6.4.文件的存储

⽂件的数据是要存储在硬盘上⾯的，数据在磁盘上的存放⽅式，就像程序在内存中存放的⽅式那样，有以下两种：

- 连续空间存放⽅式
- ⾮连续空间存放⽅式

其中，⾮连续空间存放⽅式⼜可以分为「链表⽅式」和「索引⽅式」。

不同的存储⽅式，有各⾃的特点，重点是要分析它们的存储效率和读写性能，接下来分别对每种存储⽅式说⼀下。

#### 6.4.1.连续空间存放方式

连续空间存放⽅式顾名思义，⽂件存放在磁盘「连续的」物理空间中。这种模式下，⽂件的数据都是紧密相连，读写效率很⾼，因为⼀次磁盘寻道就可以读出整个⽂件。

使⽤连续存放的⽅式有⼀个前提，必须先知道⼀个⽂件的⼤⼩，这样⽂件系统才会根据⽂件的⼤⼩在磁盘上找到⼀块连续的空间分配给⽂件。

所以，⽂件头⾥需要指定「起始块的位置」和「⻓度」，有了这两个信息就可以很好的表示⽂件存放⽅式是⼀块连续的磁盘空间。

连续空间存放的⽅式虽然读写效率⾼，但是有「磁盘空间碎⽚」和「⽂件⻓度不易扩展」的缺陷。

那么有没有更好的⽅式来解决上⾯的问题呢？答案当然有，既然连续空间存放的⽅式不太⾏，那么我们就改变存放的⽅式，使⽤⾮连续空间存放⽅式来解决这些缺陷。

#### 6.4.2.非连续空间存放方式

⾮连续空间存放⽅式分为「链表⽅式」和「索引⽅式」。

> 我们先来看看链表的⽅式。

链表的⽅式存放是离散的，不⽤连续的，于是就可以消除磁盘碎⽚，可⼤⼤提⾼磁盘空间的利⽤率，同时⽂件的⻓度可以动态扩展。根据实现的⽅式的不同，链表可分为「隐式链表」和「显式链接」两种形式。

⽂件要以「隐式链表」的⽅式存放的话，实现的⽅式是⽂件头要包含「第⼀块」和「最后⼀块」的位置，并且每个数据块⾥⾯留出⼀个指针空间，⽤来存放下⼀个数据块的位置，这样⼀个数据块连着⼀个数据块，从链头开是就可以顺着指针找到所有的数据块，所以存放的⽅式可以是不连续的。

隐式链表的存放⽅式的缺点在于⽆法直接访问数据块，只能通过指针顺序访问⽂件，以及数据块指针消耗了⼀定的存储空间。隐式链接分配的稳定性较差，系统在运⾏过程中由于软件或者硬件错误导致链表中的指针丢失或损坏，会导致⽂件数据的丢失。

如果取出每个磁盘块的指针，把它放在内存的⼀个表中，就可以解决上述隐式链表的两个不⾜。那么，这种实现⽅式是「显式链接」，它指把⽤于链接⽂件各数据块的指针，显式地存放在内存的⼀张链接表中，该表在整个磁盘仅设置⼀张，每个表项中存放链接指针，指向下⼀个数据块号。

由于查找记录的过程是在内存中进⾏的，因⽽不仅显著地提⾼了检索速度，⽽且⼤⼤减少了访问磁盘的次数。但也正是整个表都存放在内存中的关系，它的主要的缺点是不适⽤于⼤磁盘。

> 接下来，我们来看看索引的⽅式。

链表的⽅式解决了连续分配的磁盘碎⽚和⽂件动态扩展的问题，但是不能有效⽀持直接访问（FAT除外），索引的⽅式可以解决这个问题。

索引的实现是为每个⽂件创建⼀个「索引数据块」，⾥⾯存放的是指向⽂件数据块的指针列表，说⽩了就像书的⽬录⼀样，要找哪个章节的内容，看⽬录查就可以。

另外，⽂件头需要包含指向「索引数据块」的指针，这样就可以通过⽂件头知道索引数据块的位置，再通过索引数据块⾥的索引信息找到对应的数据块。

索引的⽅式优点在于：

- ⽂件的创建、增⼤、缩⼩很⽅便；
- 不会有碎⽚的问题；
- ⽀持顺序读写和随机读写；

由于索引数据也是存放在磁盘块的，如果⽂件很⼩，明明只需⼀块就可以存放的下，但还是需要额外分配⼀块来存放索引数据，所以缺陷之⼀就是存储索引带来的开销。

如果⽂件很⼤，⼤到⼀个索引数据块放不下索引信息，这时⼜要如何处理⼤⽂件的存放呢？我们可以通过组合的⽅式，来处理⼤⽂件的存储。

先来看看链表 + 索引的组合，这种组合称为「链式索引块」，它的实现⽅式是在索引数据块留出⼀个存放下⼀个索引数据块的指针，于是当⼀个索引数据块的索引信息⽤完了，就可以通过指针的⽅式，找到下⼀个索引数据块的信息。那这种⽅式也会出现前⾯提到的链表⽅式的问题，万⼀某个指针损坏了，后⾯的数据也就会⽆法读取了。

还有另外⼀种组合⽅式是索引 + 索引的⽅式，这种组合称为「多级索引块」，实现⽅式是通过⼀个索引块来存放多个索引数据块，⼀层套⼀层索引，像极了俄罗斯套娃是吧。

#### 6.4.3.Unix文件的实现方式

先把前⾯提到的⽂件实现⽅式，做个⽐较：

![](https://moon-axuan.oss-cn-beijing.aliyuncs.com/飞书20220709-110808.png)

那早期 Unix ⽂件系统是组合了前⾯的⽂件存放⽅式的优点

它是根据⽂件的⼤⼩，存放的⽅式会有所变化：

- 如果存放⽂件所需的数据块⼩于 10 块，则采⽤直接查找的⽅式；
- 如果存放⽂件所需的数据块超过 10 块，则采⽤⼀级间接索引⽅式；如果前⾯
- 两种⽅式都不够存放⼤⽂件，则采⽤⼆级间接索引⽅式；
- 如果⼆级间接索引也不够存放⼤⽂件，这采⽤三级间接索引⽅式；

所以，这种⽅式能很灵活地⽀持⼩⽂件和⼤⽂件的存放：

- 对于⼩⽂件使⽤直接查找的⽅式可减少索引数据块的开销；
- 对于⼤⽂件则以多级索引的⽅式来⽀持，所以⼤⽂件在访问数据块时需要⼤量查询；

### 6.5.空闲空间管理

前⾯说到的⽂件的存储是针对已经被占⽤的数据块组织和管理，接下来的问题是，如果我要保存⼀个数据块，我应该放在硬盘上的哪个位置呢？难道需要将所有的块扫描⼀遍，找个空的地⽅随便放吗？

那这种⽅式效率就太低了，所以针对磁盘的空闲空间也是要引⼊管理的机制

#### 6.5.1.空闲表法

空闲表法就是为所有空闲空间建⽴⼀张表，表内容包括空闲区的第⼀个块号和该空闲区的块个数，注意，这个⽅式是连续分配的。

当请求分配磁盘空间时，系统依次扫描空闲表⾥的内容，直到找到⼀个合适的空闲区域为⽌。当⽤户撤销⼀个⽂件时，系统回收⽂件空间。这时，也需顺序扫描空闲表，寻找⼀个空闲表条⽬并将释放空间的第⼀个物理块号及它占⽤的块数填到这个条⽬中。

这种⽅法仅当有少量的空闲区时才有较好的效果。因为，如果存储空间中有着⼤量的⼩的空闲区，则空闲表变得很⼤，这样查询效率会很低。另外，这种分配技术适⽤于建⽴连续⽂件。

#### 6.5.2.空闲链表法

我们也可以使⽤「链表」的⽅式来管理空闲空间，每⼀个空闲块⾥有⼀个指针指向下⼀个空闲块，这样也能很⽅便的找到空闲块并管理起来。

这种技术只要在主存中保存⼀个指针，令它指向第⼀个空闲块。其特点是简单，但不能随机访问，⼯作效率低，因为每当在链上增加或移动空闲块时需要做很多 I/O 操作，同时数据块的指针消耗了⼀定的存储空间。

空闲表法和空闲链表法都不适合⽤于⼤型⽂件系统，因为这会使空闲表或空闲链表太⼤。

#### 6.5.3.位图法

位图是利⽤⼆进制的⼀位来表示磁盘中⼀个盘块的使⽤情况，磁盘上所有的盘块都有⼀个⼆进制位与之对应。

当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。它形式如下：

1111110011111110001110110111111100111 ...

在 Linux ⽂件系统就采⽤了位图的⽅式来管理空闲空间，不仅⽤于数据空闲块的管理，还⽤于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，⾃然也要有对其管理。

### 6.6.文件I/O

#### 6.6.1.缓冲与非缓冲I/O

⽂件操作的标准库是可以实现数据的缓存，那么根据「是否利⽤标准库缓冲」，可以把⽂件 I/O 分为缓冲I/O 和⾮缓冲 I/O：

- 缓冲 I/O，利⽤的是标准库的缓存实现⽂件的加速访问，⽽标准库再通过系统调⽤访问⽂件。
- ⾮缓冲 I/O，直接通过系统调⽤访问⽂件，不经过标准库缓存。

这⾥所说的「缓冲」特指标准库内部实现的缓冲。

⽐⽅说，很多程序遇到换⾏时才真正输出，⽽换⾏前的内容，其实就是被标准库暂时缓存了起来，这样做的⽬的是，减少系统调⽤的次数，毕竟系统调⽤是有 CPU 上下⽂切换的开销的。

#### 6.6.2.直接与非直接I/O

我们都知道磁盘 I/O 是⾮常慢的，所以 Linux 内核为了减少磁盘 I/O 次数，在系统调⽤后，会把⽤户数据拷⻉到内核中缓存起来，这个内核缓存空间也就是「⻚缓存」，只有当缓存满⾜某些条件的时候，才发起磁盘 I/O 的请求。

那么，根据是「否利⽤操作系统的缓存」，可以把⽂件 I/O 分为直接 I/O 与⾮直接 I/O：

- 直接 I/O，不会发⽣内核缓存和⽤户程序之间数据复制，⽽是直接经过⽂件系统访问磁盘。
- ⾮直接 I/O，读操作时，数据从内核缓存中拷⻉给⽤户程序，写操作时，数据从⽤户程序拷⻉给内核缓存，再由内核决定什么时候写⼊数据到磁盘。

如果你在使⽤⽂件操作类的系统调⽤函数时，指定了 O_DIRECT 标志，则表示使⽤直接 I/O。如果没有设置过，默认使⽤的是⾮直接 I/O。

>  如果⽤了⾮直接 I/O 进⾏写数据操作，内核什么情况下才会把缓存数据写⼊到磁盘？

以下⼏种场景会触发内核缓存的数据写⼊磁盘：

- 在调⽤ write 的最后，当发现内核缓存的数据太多的时候，内核会把数据写到磁盘上；
- ⽤户主动调⽤ sync ，内核缓存会刷到磁盘上；
- 当内存⼗分紧张，⽆法再分配⻚⾯时，也会把内核缓存的数据刷到磁盘上；
- 内核缓存的数据的缓存时间超过某个时间时，也会把数据刷到磁盘上；

#### 6.6.6.3.阻塞与非阻塞I/O VS 同步与异步I/O

为什么把阻塞 / ⾮阻塞与同步与异步放⼀起说的呢？因为它们确实⾮常相似，也⾮常容易混淆，不过它们之间的关系还是有点微妙的。

先来看看阻塞 I/O，当⽤户程序执⾏ read ，线程会被阻塞，⼀直等到内核数据准备好，并把数据从内核缓冲区拷⻉到应⽤程序的缓冲区中，当拷⻉过程完成， read 才会返回。

注意，阻塞等待的是「内核数据准备好」和「数据从内核态拷⻉到⽤户态」这两个过程。

⾮阻塞 I/O，⾮阻塞的 read 请求在数据未准备好的情况下⽴即返回，可以继续往
下执⾏，此时应⽤程序不断轮询内核，直到数据准备好，内核将数据拷⻉到应⽤程序缓冲区， read 调⽤才可以获取到结果。

注意，`这⾥最后⼀次 read 调⽤，获取数据的过程，是⼀个同步的过程，是需要等待的过程。这⾥的同步指的是内核态的数据拷⻉到⽤户程序的缓存区这个过程`。

应⽤程序每次轮询内核的 I/O 是否准备好，感觉有点傻乎乎，因为轮询的过程中，应⽤程序啥也做不了，只是在循环。

为了解决这种傻乎乎轮询⽅式，于是 I/O 多路复⽤技术就出来了，如 select、poll，它是通过 I/O 事件分发，当内核数据准备好时，再以事件通知应⽤程序进⾏操作。

这个做法⼤⼤改善了应⽤进程对 CPU 的利⽤率，在没有被通知的情况下，应⽤进程可以使⽤ CPU 做其他的事情。使⽤ select I/O 多路复⽤， read 获取数据的过程（数据从内核态拷⻉到⽤户态的过程），也是⼀个同步的过程，需要等待。

实际上，⽆论是阻塞 I/O、⾮阻塞 I/O，还是基于⾮阻塞 I/O 的多路复⽤都是同步调⽤。因为它们在 read调⽤时，内核将数据从内核空间拷⻉到应⽤程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷⻉效率不⾼，read 调⽤就会在这个同步过程中等待⽐较⻓的时间。

⽽真正的异步 I/O 是「内核数据准备好」和「数据从内核态拷⻉到⽤户态」这两个过程都不⽤等待。

当我们发起 aio_read 之后，就⽴即返回，内核⾃动将数据从内核空间拷⻉到应⽤程序空间，这个拷⻉过程同样是异步的，内核⾃动完成的，和前⾯的同步操作不⼀样，应⽤程序并不需要主动发起拷⻉动作。

## 7.设备管理

键盘可以说是我们最常使⽤的输⼊硬件设备了，但身为程序员的你，你知道「键盘敲⼊A 字⺟时，操作系
统期间发⽣了什么吗」？

![](https://moon-axuan.oss-cn-beijing.aliyuncs.com/飞书20220709-140813.png)

CPU ⾥⾯的内存接⼝，直接和系统总线通信，然后系统总线再接⼊⼀个 I/O 桥接器，这个 I/O 桥接器，另⼀边接⼊了内存总线，使得 CPU 和内存通信。再另⼀边，⼜接⼊了⼀个 I/O 总线，⽤来连接 I/O 设备，⽐如键盘、显示器等。

那当⽤户输⼊了键盘字符，键盘控制器就会产⽣扫描码数据，并将其缓冲在键盘控制器的寄存器中，紧接着键盘控制器通过总线给 CPU 发送中断请求。

CPU 收到中断请求后，操作系统会保存被中断进程的 CPU 上下⽂，然后调⽤键盘的中断处理程序。

键盘的中断处理程序是在键盘驱动程序初始化时注册的，那键盘中断处理函数的功能就是从键盘控制器的寄存器的缓冲区读取扫描码，再根据扫描码找到⽤户在键盘输⼊的字符，如果输⼊的字符是显示字符，那就会把扫描码翻译成对应显示字符的 ASCII 码，⽐如⽤户在键盘输⼊的是字⺟ A，是显示字符，于是就会
把扫描码翻译成 A 字符的 ASCII 码。

得到了显示字符的 ASCII 码后，就会把 ASCII 码放到「读缓冲区队列」，接下来就是要把显示字符显示屏幕了，显示设备的驱动程序会定时从「读缓冲区队列」读取数据放到「写缓冲区队列」，最后把「写缓冲区队列」的数据⼀个⼀个写⼊到显示设备的控制器的寄存器中的数据缓冲区，最后将这些数据显示在屏幕⾥。

显示出结果后，恢复被中断进程的上下⽂。

## 8.网络系统

### 8.1.Linux系统时如何收发网络包的？

电脑与电脑之间通常都是通话⽹卡、交换机、路由器等⽹络设备连接到⼀起，那由于⽹络设备的异构性，国际标准化组织定义了⼀个七层的 OSI ⽹络模型，但是这个模型由于⽐较复杂，实际应⽤中并没有采⽤，⽽是采⽤了更为简化的 TCP/IP 模型，Linux ⽹络协议栈就是按照了该模型来实现的。

TCP/IP 模型主要分为应⽤层、传输层、⽹络层、⽹络接⼝层四层，每⼀层负责的职责都不同，这也是Linux ⽹络协议栈主要构成部分。

当应⽤程序通过 Socket 接⼝发送数据包，数据包会被⽹络协议栈从上到下进⾏逐层处理后，才会被送到⽹卡队列中，随后由⽹卡将⽹络包发送出去。

⽽在接收⽹络包时，同样也要先经过⽹络协议栈从下到上的逐层处理，最后才会被送到应⽤程序。

### 8.2.零拷贝

早期 I/O 操作，内存与磁盘的数据传输的⼯作都是由 CPU 完成的，⽽此时 CPU 不能执⾏其他任务，会特别浪费 CPU 资源。

![](https://moon-axuan.oss-cn-beijing.aliyuncs.com/飞书20220709-141714.png)

于是，为了解决这⼀问题，DMA 技术就出现了，每个 I/O 设备都有⾃⼰的 DMA 控制器，通过这个 DMA控制器，CPU 只需要告诉 DMA 控制器，我们要传输什么数据，从哪⾥来，到哪⾥去，就可以放⼼离开了。后续的实际数据传输⼯作，都会由 DMA 控制器来完成，CPU 不需要参与数据传输的⼯作。传统 IO 的⼯作⽅式，从硬盘读取数据，然后再通过⽹卡向外发送，我们需要进⾏ 4 上下⽂切换，和 4 次数据拷⻉，其中 2 次数据拷⻉发⽣在内存⾥的缓冲区和对应的硬件设备之间，这个是由 DMA 完成，另外2 次则发⽣在内核态和⽤户态之间，这个数据搬移⼯作是由 CPU 完成的。

为了提⾼⽂件传输的性能，于是就出现了零拷⻉技术，它通过⼀次系统调⽤（ sendfile ⽅法）合并了磁盘读取与⽹络发送两个操作，降低了上下⽂切换次数。另外，拷⻉数据都是发⽣在内核中的，天然就降低了数据拷⻉的次数。

--------------------------------------

**如何实现零拷贝？**

> mmap + write

read() 系统调⽤的过程中会把内核缓冲区的数据拷⻉到⽤户的缓冲区⾥，于是为了减少这⼀步开销，我们可以⽤ mmap() 替换 read() 系统调⽤函数。

mmap() 系统调⽤函数会直接把内核缓冲区⾥的数据「映射」到⽤户空间，这样，操作系统内核与⽤户空间就不需要再进⾏任何的数据拷⻉操作。

![](https://moon-axuan.oss-cn-beijing.aliyuncs.com/飞书20220709-143249.png)

具体过程如下：

- 应⽤进程调⽤了 mmap() 后，DMA 会把磁盘的数据拷⻉到内核的缓冲区⾥。接着，应⽤进程跟操作系统内核「共享」这个缓冲区；
- 应⽤进程再调⽤ write() ，操作系统直接将内核缓冲区的数据拷⻉到 socket 缓冲区中，这⼀切都发⽣在内核态，由 CPU 来搬运数据；
- 最后，把内核的 socket 缓冲区⾥的数据，拷⻉到⽹卡的缓冲区⾥，这个过程是由 DMA 搬运的。

我们可以得知，通过使⽤ mmap() 来代替 read() ， 可以减少⼀次数据拷⻉的过程。

但这还不是最理想的零拷⻉，因为仍然需要通过 CPU 把内核缓冲区的数据拷⻉到 socket 缓冲区⾥，⽽且仍然需要 4 次上下⽂切换，因为系统调⽤还是 2 次。

> sendfile

在 Linux 内核版本 2.1 中，提供了⼀个专⻔发送⽂件的系统调⽤函数 sendfile()

⾸先，它可以替代前⾯的 read() 和 write() 这两个系统调⽤，这样就可以减少⼀次系统调⽤，也就减少了 2 次上下⽂切换的开销。

其次，该系统调⽤，可以直接把内核缓冲区⾥的数据拷⻉到 socket 缓冲区⾥，不再拷⻉到⽤户态，这样就只有 2 次上下⽂切换，和 3 次数据拷⻉。

![](https://moon-axuan.oss-cn-beijing.aliyuncs.com/飞书20220709-143521.png)

但是这还不是真正的零拷⻉技术，如果⽹卡⽀持 SG-DMA（The Scatter-Gather Direct Memory Access）技术（和普通的 DMA 有所不同），我们可以进⼀步减少通过 CPU 把内核缓冲区⾥的数据拷⻉到 socket缓冲区的过程。

从 Linux 内核 2.4 版本开始起，对于⽀持⽹卡⽀持 SG-DMA 技术的情况下， sendfile() 系统调⽤的过程发⽣了点变化，具体过程如下：

- 第⼀步，通过 DMA 将磁盘上的数据拷⻉到内核缓冲区⾥；
- 第⼆步，缓冲区描述符和数据⻓度传到 socket 缓冲区，这样⽹卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷⻉到⽹卡的缓冲区⾥，此过程不需要将数据从操作系统内核缓冲区拷⻉到 socket缓冲区中，这样就减少了⼀次数据拷⻉；

所以，这个过程之中，只进⾏了 2 次数据拷⻉

![](https://moon-axuan.oss-cn-beijing.aliyuncs.com/飞书20220709-143848.png)

这就是所谓的零拷⻉（Zero-copy）技术，因为我们没有在内存层⾯去拷⻉数据，也就是说全程没有通过CPU 来搬运数据，所有的数据都是通过 DMA 来进⾏传输的。

零拷⻉技术的⽂件传输⽅式相⽐传统⽂件传输的⽅式，减少了 2 次上下⽂切换和数据拷⻉次数，只需要 2次上下⽂切换和数据拷⻉次数，就可以完成⽂件的传输，⽽且 2 次的数据拷⻉过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。

所以，总体来看，零拷⻉技术可以把⽂件传输的性能提⾼⾄少⼀倍以上。

---

Kafka 和 Nginx 都有实现零拷⻉技术，这将⼤⼤提⾼⽂件传输的性能。

零拷⻉技术是基于 PageCache 的，PageCache 会缓存最近访问的数据，提升了访问缓存数据的性能，同时，为了解决机械硬盘寻址慢的问题，它还协助 I/O 调度算法实现了 IO 合并与预读，这也是顺序读⽐随机读性能好的原因。这些优势，进⼀步提升了零拷⻉的性能。

需要注意的是，零拷⻉技术是不允许进程对⽂件内容作进⼀步的加⼯的，⽐如压缩数据再发送。

另外，当传输⼤⽂件时，不能使⽤零拷⻉，因为可能由于 PageCache 被⼤⽂件占据，⽽导致「热点」⼩⽂件⽆法利⽤到 PageCache，并且⼤⽂件的缓存命中率不⾼，这时就需要使⽤「异步 IO + 直接 IO 」的⽅式。

在 Nginx ⾥，可以通过配置，设定⼀个⽂件⼤⼩阈值，针对⼤⽂件使⽤异步 IO 和直接 IO，⽽对⼩⽂件使⽤零拷⻉。

### 8.3.I/O 多路复用：select/poll/epoll

最基础的 TCP 的 Socket 编程，它是阻塞 I/O 模型，基本上只能⼀对⼀通信，那为了服务更多的客户端，我们需要改进⽹络 I/O 模型。

⽐较传统的⽅式是使⽤多进程/线程模型，每来⼀个客户端连接，就分配⼀个进程/线程，然后后续的读写都在对应的进程/线程，这种⽅式处理 100 个客户端没问题，但是当客户端增⼤到 10000 个时，10000 个进程/线程的调度、上下⽂切换以及它们占⽤的内存，都会成为瓶颈。

为了解决上⾯这个问题，就出现了 I/O 的多路复⽤，可以只在⼀个进程⾥处理多个⽂件的 I/O，Linux 下有三种提供 I/O 多路复⽤的 API，分别是： select、poll、epoll。

select 和 poll 并没有本质区别，它们内部都是使⽤「线性结构」来存储进程关注的 Socket 集合。

在使⽤的时候，⾸先需要把关注的 Socket 集合通过 select/poll 系统调⽤从⽤户态拷⻉到内核态，然后由内核检测事件，当有⽹络事件产⽣时，内核需要遍历进程关注 Socket 集合，找到对应的 Socket，并设置其状态为可读/可写，然后把整个 Socket 集合从内核态拷⻉到⽤户态，⽤户态还要继续遍历整个 Socket 集合找到可读/可写的 Socket，然后对其处理。

很明显发现，select 和 poll 的缺陷在于，当客户端越多，也就是 Socket 集合越⼤，Socket 集合的遍历和拷⻉会带来很⼤的开销，因此也很难应对 C10K。

epoll 是解决 C10K 问题的利器，通过两个⽅⾯解决了 select/poll 的问题。

- epoll 在内核⾥使⽤「红⿊树」来关注进程所有待检测的 Socket，红⿊树是个⾼效的数据结构，增删查⼀般时间复杂度是 O(logn)，通过对这棵⿊红树的管理，不需要像 select/poll 在每次操作时都传⼊整个 Socket 集合，减少了内核和⽤户空间⼤量的数据拷⻉和内存分配。
- epoll 使⽤事件驱动的机制，内核⾥维护了⼀个「链表」来记录就绪事件，只将有事件发⽣的 Socket集合传递给应⽤程序，不需要像 select/poll 那样轮询扫描整个集合（包含有和⽆事件的 Socket ），⼤⼤提⾼了检测的效率。

---

> 边缘触发和水平触发？

这两个术语还挺抽象的，其实它们的区别还是很好理解的。

- 使⽤边缘触发模式时，当被监控的 Socket 描述符上有可读事件发⽣时，服务器端只会从 epoll_wait中苏醒⼀次，即使进程没有调⽤ read 函数从内核读取数据，也依然只苏醒⼀次，因此我们程序要保证⼀次性将内核缓冲区的数据读取完；
- 使⽤⽔平触发模式时，当被监控的 Socket 上有可读事件发⽣时，服务器端不断地从 epoll_wait 中苏
  醒，直到内核缓冲区数据被 read 函数读完才结束，⽬的是告诉我们有数据需要读取；

---

⽽且，epoll ⽀持边缘触发和⽔平触发的⽅式，⽽ select/poll 只⽀持⽔平触发，⼀般⽽⾔，边缘触发的⽅式会⽐⽔平触发的效率⾼。

### 8.4.高性能网络模式：Reactor和Proactor

常⻅的 Reactor 实现⽅案有三种。

第⼀种⽅案单 Reactor 单进程 / 线程，不⽤考虑进程间通信以及数据同步的问题，因此实现起来⽐较简单，这种⽅案的缺陷在于⽆法充分利⽤多核 CPU，⽽且处理业务逻辑的时间不能太⻓，否则会延迟响应，所以不适⽤于计算机密集型的场景，适⽤于业务处理快速的场景，⽐如 Redis 采⽤的是单 Reactor 单进程的⽅案。

第⼆种⽅案单 Reactor 多线程，通过多线程的⽅式解决了⽅案⼀的缺陷，但它离⾼并发还差⼀点距离，差在只有⼀个 Reactor 对象来承担所有事件的监听和响应，⽽且只在主线程中运⾏，在⾯对瞬间⾼并发的场景时，容易成为性能的瓶颈的地⽅。

第三种⽅案多 Reactor 多进程 / 线程，通过多个 Reactor 来解决了⽅案⼆的缺陷，主 Reactor 只负责监听事件，响应事件的⼯作交给了从 Reactor，Netty 和 Memcache 都采⽤了「多 Reactor 多线程」的⽅案，Nginx 则采⽤了类似于 「多 Reactor 多进程」的⽅案。

Reactor 可以理解为「来了事件操作系统通知应⽤进程，让应⽤进程来处理」，⽽ Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应⽤进程」。

因此，真正的⼤杀器还是 Proactor，它是采⽤异步 I/O 实现的异步⽹络模型，感知的是已完成的读写事件，⽽不需要像 Reactor 感知到事件后，还需要调⽤ read 来从内核中获取数据。

不过，⽆论是 Reactor，还是 Proactor，都是⼀种基于「事件分发」的⽹络编程模式，区别在于 Reactor模式是基于「待完成」的 I/O 事件，⽽ Proactor 模式则是基于「已完成」的 I/O 事件。